{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from tqdm.notebook import tqdm\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose,Resize,ToTensor,Normalize,RandomHorizontalFlip,RandomCrop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "patch_size = 16\n",
    "latent_size = 768\n",
    "n_channels = 3\n",
    "num_heads = 12\n",
    "num_encoders = 12\n",
    "dropout = 0.1\n",
    "num_classes = 10\n",
    "size = 224\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "base_lr = 10e-3\n",
    "weight_decay = 0.03\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Input Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self,patch_size=patch_size,n_channels=n_channels,device=device,latent_size=latent_size,batch_size=batch_size):\n",
    "        super(InputEmbedding,self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = self.patch_size * self.patch_size * self.n_channels\n",
    "        \n",
    "        # Linear Projection\n",
    "        self.linearProjection = nn.Linear(self.input_size,self.latent_size)\n",
    "        \n",
    "        # class token\n",
    "        self.class_token = nn.Parameter(torch.randn(self.batch_size,1,self.latent_size)).to(device)\n",
    "        \n",
    "        # positional embedding \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(self.batch_size,1,self.latent_size)).to(device)\n",
    "    \n",
    "    def forward(self,input_data):\n",
    "        input_data = input_data.to(device)\n",
    "        \n",
    "        # Patchify input image\n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)',h1 = self.patch_size,w1 = self.patch_size)\n",
    "        \n",
    "        # print(input_data.size())\n",
    "        # print(patches.size())\n",
    "        \n",
    "        linear_projection = self.linearProjection(patches).to(device)\n",
    "        b, n, _ = linear_projection.shape\n",
    "        \n",
    "        linear_projection = torch.cat((self.class_token,linear_projection),dim=1)\n",
    "        pos_embed = einops.repeat(self.pos_embedding, 'b 1 d -> b m d',m=n+1)\n",
    "        # print(linear_projection.size())\n",
    "        # print(pos_embed.size())\n",
    "        \n",
    "        linear_projection += pos_embed\n",
    "        \n",
    "        return linear_projection\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn((8,3,224,224))\n",
    "test_class = InputEmbedding().to(device)\n",
    "embed_test = test_class(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,latent_size=latent_size,num_heads=num_heads,device=device,dropout=dropout):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Normalization layer \n",
    "        self.norm = nn.LayerNorm(self.latent_size)\n",
    "        \n",
    "        self.multihead = nn.MultiheadAttention(\n",
    "            self.latent_size,self.num_heads,self.dropout\n",
    "        )\n",
    "        \n",
    "        self.enc_MLP = nn.Sequential(\n",
    "            nn.Linear(self.latent_size,self.latent_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.latent_size*4,self.latent_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,embedded_pathes):\n",
    "        firstnorm = self.norm(embedded_pathes)\n",
    "        attention_out = self.multihead(firstnorm,firstnorm,firstnorm)[0]\n",
    "        \n",
    "        # first residual connection\n",
    "        first_added = attention_out + embedded_pathes\n",
    "        \n",
    "        secondnorm_out = self.norm(first_added)\n",
    "        ff_out = self.enc_MLP(secondnorm_out)\n",
    "        \n",
    "        # output = ff_out + first_added\n",
    "        # print('Embed: ',embedded_pathes.size())\n",
    "        # print('Output: ',output.size())\n",
    "        \n",
    "        return ff_out + first_added    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5248, -0.0131, -0.8531,  ...,  0.0661,  0.4559, -0.1604],\n",
       "         [-1.3620, -0.3457,  0.7769,  ...,  0.9863,  0.3427,  0.2748],\n",
       "         [-0.3905,  0.5816, -0.5273,  ...,  2.2097, -0.2858,  0.6337],\n",
       "         ...,\n",
       "         [ 0.0492,  1.3259, -1.1039,  ...,  0.7285, -0.4145, -0.5071],\n",
       "         [-0.3874,  0.5646, -0.2843,  ...,  1.9877, -0.5334, -0.6394],\n",
       "         [-0.0396, -0.5293,  0.6363,  ...,  0.4858,  0.4059,  0.2588]],\n",
       "\n",
       "        [[-1.4182, -2.7925,  2.5594,  ..., -0.7089,  0.3096,  2.5393],\n",
       "         [-0.8160, -1.4472,  1.5562,  ...,  0.2870,  0.8329,  1.3774],\n",
       "         [-2.2243, -1.3191,  2.2585,  ...,  0.6834,  2.5762,  1.8160],\n",
       "         ...,\n",
       "         [-0.9010, -2.0125,  2.2293,  ..., -0.4387,  1.7208,  2.1022],\n",
       "         [ 0.0475, -1.7603, -0.0063,  ..., -0.4887,  1.7121,  1.7676],\n",
       "         [-1.3061, -3.3139,  1.8482,  ..., -0.6609,  1.3545,  1.2455]],\n",
       "\n",
       "        [[-1.6195,  2.5127, -0.4175,  ..., -2.7350, -0.4364,  1.1066],\n",
       "         [-1.4524,  1.9297, -0.4958,  ..., -0.6274,  2.6637,  1.3066],\n",
       "         [-1.4808,  1.7084, -0.2761,  ..., -0.3434,  2.7054,  0.3516],\n",
       "         ...,\n",
       "         [ 0.0322,  0.8092,  0.4247,  ..., -0.0971,  2.1358,  0.4675],\n",
       "         [-1.1345,  0.5612, -0.3980,  ..., -0.5105,  2.1677,  0.8505],\n",
       "         [-1.0158,  1.0373, -1.3318,  ..., -0.3453,  1.3678,  0.9879]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0716,  0.5499, -0.9212,  ..., -0.7496,  0.4948, -2.5362],\n",
       "         [ 0.3134, -0.8742,  0.4484,  ..., -0.6778, -1.3421, -1.2447],\n",
       "         [-0.6174, -0.6874, -0.5457,  ...,  0.3500, -0.6628, -0.7530],\n",
       "         ...,\n",
       "         [-0.9545, -0.7875,  0.6310,  ..., -0.6818, -0.7060, -1.5441],\n",
       "         [ 0.4433, -1.6618, -0.1818,  ...,  0.0539, -0.7964, -0.6580],\n",
       "         [ 0.4792, -1.3148,  0.0388,  ..., -0.6337, -1.7234, -1.2159]],\n",
       "\n",
       "        [[-1.4426,  2.5226,  0.6379,  ...,  1.0859,  1.0896, -0.0337],\n",
       "         [-0.7423,  0.7196,  0.9563,  ...,  0.1553,  0.2566, -0.1512],\n",
       "         [-1.8719, -0.2793,  0.4528,  ..., -0.9962,  0.3286,  0.8180],\n",
       "         ...,\n",
       "         [-1.4083,  0.6838, -0.4559,  ..., -0.9271,  0.6261,  0.9588],\n",
       "         [-1.3870,  0.5936,  0.2938,  ...,  0.1726,  0.0908,  0.5972],\n",
       "         [-1.5362,  0.4834,  1.7829,  ...,  0.2546, -0.7841,  0.3163]],\n",
       "\n",
       "        [[ 0.7353,  1.2934, -0.9427,  ..., -1.9406, -1.4931, -0.5597],\n",
       "         [ 1.9282,  2.6678, -1.7015,  ..., -0.6018,  1.0078, -0.1520],\n",
       "         [ 0.6662,  1.8724, -0.1274,  ..., -1.0555,  1.2472,  0.7687],\n",
       "         ...,\n",
       "         [ 1.1960,  1.3600, -0.0156,  ..., -0.8084,  1.1119,  0.2001],\n",
       "         [ 0.0064,  1.6866, -0.4288,  ..., -0.8399,  0.1476,  1.4527],\n",
       "         [ 0.7244,  2.5118, -0.2074,  ...,  0.0165,  0.1205,  1.3659]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoder = Encoder().to(device)\n",
    "test_encoder(embed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,num_encoders=num_encoders,latent_size=latent_size,device=device,num_classes=num_classes,dropout=dropout):\n",
    "        super(ViT,self).__init__()\n",
    "        self.num_encoder = num_encoders\n",
    "        self.latent_size = latent_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = InputEmbedding()\n",
    "        \n",
    "        # create the stack of encoders\n",
    "        self.encStack = nn.ModuleList([Encoder() for i in range(self.num_encoder)])\n",
    "        \n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.latent_size),\n",
    "            nn.Linear(self.latent_size,self.latent_size),\n",
    "            nn.Linear(self.latent_size,self.num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self,test_input):\n",
    "        enc_output = self.embedding(test_input)\n",
    "        \n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer(enc_output)\n",
    "        \n",
    "        cls_token_embed = enc_output[:,0]\n",
    "        \n",
    "        return self.MLP_head(cls_token_embed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3254, -0.1509,  0.2518,  0.1005, -0.0672,  0.1720, -0.2237,  0.0961,\n",
      "         -0.2804, -0.3799],\n",
      "        [-0.0868, -0.1790, -0.0209,  0.0729,  0.2386,  0.1178, -0.1998, -0.1146,\n",
      "         -0.1288,  0.1116],\n",
      "        [ 0.1914,  0.3783,  0.4936,  0.1537, -0.2746, -0.4081, -0.0298,  0.3294,\n",
      "         -0.1753, -0.1010],\n",
      "        [ 0.0350, -0.3818, -0.4114, -0.0839,  0.0546, -0.0980, -0.1402,  0.2382,\n",
      "         -0.1924,  0.0042],\n",
      "        [ 0.1217,  0.3642,  0.0122, -0.1125,  0.5475, -0.3069,  0.3059,  0.2368,\n",
      "         -0.6082, -0.1529],\n",
      "        [ 0.0226, -0.1642,  0.1978, -0.5831, -0.3268, -0.3327, -0.3731,  0.4667,\n",
      "         -0.2083, -0.0684],\n",
      "        [ 0.2539,  0.1225,  0.7804,  0.5862,  0.0316, -0.3312,  0.1216,  0.5738,\n",
      "         -0.6443,  0.0989],\n",
      "        [ 0.4382,  0.3348, -0.1298,  0.0220,  0.5685,  0.4743,  0.1789,  0.3369,\n",
      "          0.0847,  0.1542]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "model = ViT().to(device)\n",
    "vit_output = model(test_input)\n",
    "print(vit_output)\n",
    "print(vit_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
